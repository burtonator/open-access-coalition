<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
<title>Open Access Coalition</title>
<link rel="stylesheet" href="default.css"></link>
    
</head>
<body>
<div id="wrap">

<a href="/"><img src="logo.png" /></a>

<div class="navbar">
<a href="/">Home</a>
<a href="faq.html">FAQ</a>
<a href="http://blog.openaccesscoalition.org" target="_new">Weblog</a>
<a href="https://groups.google.com/forum/?fromgroups#!forum/oac-general" target="_new">Group</a>
</div>

<div id="content">

<h1>Our goal is to fight for the open web!</h1>

<a name="open-web"/>

<h2>What is the open web?</h2>

<p>The open web is based on the concept that websites participate fairly with
   one another and allow data to be published and accessed without being
   blocked by a walled garden of technical obstacles or legal threats.
</p>

<p>
    Without the open web, a start-up like Google, which gathers content
    across the whole web, indexes it, and supports search results, would
    never have been able to get off the ground.
</p>

<p>Much of this user-generated data is published within the public domain,
available under fair use, or Creative Commons license. </p>

<p>Problems arise when publicly viewable data is then selectively made
accessible to some and denied access to by others on a discriminatory basis.</p>

<h2>Why should I care about the open web?</h2>

<p>An open web is critical to the future of the Internet and innovation. Data is
the air that web startups breath and without it all innovation on the web will
die.</p>

<p>Users depend on this innovation. The entire social media monitoring industry,
a billion dollar industry, completely relies on open and fair access to web
data.</p>

<p>Users have rights around their own data. They have the right to export this
data, the right to share it with other companies, and the right to benefit when
new companies build innovative products around their data.</p>

<h2>What is non-discriminatory access?</h2>

<a name="non-discriminatory"/>
<p>
A major component to a fair access policy is non-discriminatory access.  It's
paramount that a hosting provider treat each party in an identical manner.  The
access rules must remain consistent if access is to be judged fair and reasonable.
</p>

<p>We have seen web publishers create APIs to build an ecosystem of developers
around their platform of public data, but such access being arbitrarily
terminated to particular parties that become too successful in adjacent
marketplaces that a publishers wants to protect. Manipulating such access to
public data to thwarts innovation and competition, and is inherently detrimental
to both users and society as a whole. </p>

<p> This is a chilling effect because it prevents companies from investing
significantly in an API because they may be cut off at any time. </p>

<p>This is very similar to the way patents are licensed within open standards
using <a href="http://en.wikipedia.org/wiki/Reasonable_and_non-discriminatory_licensing">FRAND (Fair Reasonable and Non-discriminatory Licensing)</a>
</p>

<p>
<blockquote>
Non-discriminatory relates to both the terms and the rates included in licensing
agreements. As the name suggests this commitment requires that licensors treat
each individual licensee in a similar manner. This does not mean that the rates
and payment terms can't change dependent on the volume and creditworthiness of
the licensee. However it does mean that the underlying licensing condition
included in a licensing agreement must be the same regardless of the licensee.
This obligation is included in order to maintain a level playing field with
respect to existing competitors and to ensure that potential new entrants are
free to enter the market on the same basis.
</blockquote>
</p>

<p> This often happens with web crawlers where data is given to Google and
Microsoft but not to anyone else. </p>

<p>
Without non-discriminatory access, the web is not open.
</p>

<h2>Don't these websites OWN the data?</h2>

<p>Yes and no.  Very few websites own the data submitted by the user.  They often
request permissions to use the data within their application but do not attempt to
assert exclusive copyright on their user generated content.</p>

<p>The copyright on the data is owned by the user and often under the public
domain, a Creative Commons license, or available for access under doctrines of
fair use or implied license.</p>

<p>Furthermore, some particularly unfair (but legal) TOUs are written in such
voluminous and dense legalize that no one can understand just what is and is
not permissible regarding access to data or interoperation on a site. The TOUs
can then be arbitrarily interpreted in any way the publisher so deems --
ensuring that equally arbitrary termination, blocking, or suing are all chilling
threats held over the heads of third parties that wish to work with the data of
a particular site.</p>

<p>Recently, <a href="https://www.eff.org/deeplinks/2012/08/good-news-craigslist-drops-exclusive-license-your-posts">Craigslist decided to (shortly) assert exclusive copyright on user
generated content</a> but then did the right thing and backed down from that position:</p>

<p>
<blockquote>

<p>
 In a welcome course correction, craigslist has removed its short-lived provision that required users to grant it an exclusive license to--in other words granting them ownership of--every post. We were unhappily surprised to see this click-through demand, but are glad to see that craigslist has promptly removed it. 
</p>
 
<p>
 For many years, craigslist has been a good digital citizen. Its opposition to SOPA/PIPA was critically important, and it has been at the forefront of challenges to Section 230 and freedom of expression online. We understand that craigslist faces real challenges in trying to preserve its character and does not want third parties to simply reuse its content in ways that are out of line with its user community's expectations and could be harmful to its users. 
</p>

<p>
 Nevertheless, it was important for craigslist to remove the provision because claiming an exclusive license to the user's posts--to the exclusion of everyone, including the original poster--would have harmed both innovation and users' rights, and would have set a terrible precedent. We met with craigslist to discuss this recently and are pleased about their prompt action.  
</p>

</blockquote>
</p>

<h2>Don't crawlers and use excessive resources when indexing a site?</h2>

<p>No.  Not fair crawlers.</p>

<p>We want to make it clear that we do not support or defend excessive resource
utilization used by misbehaving crawlers and robots. These systems cause havoc
and are very costly to many websites.</p>

<p>However, for large sites, fetching the home page or an RSS feed of public
content imposes a very insignificant impact on the site's performance. For a
website receiving 1M hits per day, fetching the home page once per hour
increases the load on the website by 0.01%.</p>

<p>Further, if the site were to publish an update stream via
<a href="https://code.google.com/p/pubsubhubbub/">pubsub</a> or an
<a href="http://www.sixapart.com/labs/update/developers/">update stream</a>
the resouce utilization is almost zero.
</p>

<p>For years, <a href="http://sixapart.com">Six Apart</a> was publishing a
stream of updates to anyone who wanted access, free of charge, nearly
spam-free, and with non-discriminatory access to all parties.</p>

<p>They also went further and provided the feed to anyone who wanted it -
immediately.  They also removed private posts from the stream to minimize user
concerns about their private posts being indexed by search engines.</p>

<p>
Often, the resource utilization issue is a red herring used as an excuse to
continue unfair access policies.
</p>

<p>They still provide access to the same feed but have migrated to a more
modern protocol - pubsub.</p>

<h2>Don't hosting providers have the right to publish content under any terms they desire?</h2>

<p>
Yes.  A hosting provider is well within their rights to publish
data under any license they choose.  This could range from being completely unfair <i>or</i>
<b>very</b> fair.
</p>

<p> Users have the right to know in plain terms about the access policies of
sites they use, similar to the way that responsible websites publish clear cut
privacy policies. </p>

<p>Further, the OAC and the user community is free to criticize these sites and
encourage users to take their business elsewhere. </p>

<p>We believe it's in the users best interest to insist on using sites that have
fair access. Additionally, we feel it's vital to the growth of the industry to
have a free, fair, and open Internet. </p>

<h2>What are some common examples that violate fair access?</h2>

<h3>Blocking all crawlers via robots.txt - except Google:</h3>

<p>This happens far too often. This is a problem for new search engines because
they have to make a decision about whether to yield to the robots.txt block,
contact the website owner (which could involve thousands of websites) or
providing a sub-standard experience to their users. Far too often, a decision to
compete in the same way that Google behaves, becomes a contractual, legal, and
even criminal violation.</p>

<h3>Differential pricing based on size, or hidden partnerships:</h3>

<p>There have been situations where API licenses are given based on hidden
partnerships, private investments, or size of company which have prevented open
use of the data.</p>

<p>One example is Twitter's firehose license to Google and Microsoft.</p>

<p>Twitter licensed their firehose to Google (for millions of dollars)
and Google eventually didn't like the terms and <a href="http://allthingsd.com/20110715/with-google-gone-for-now-twitter-tries-to-come-to-terms-with-microsofts-bing/">cancelled their license</a>.
</p>

<p>The issue seems to be that Twitter was attempting to charge Google an
excessive price for a full license for which Google refused.</p>

<h3>Harassment:</h3>

<p>There have also been situations where large social networks have harassed
both individuals and small companies with large lawsuits or threat of lawsuit
simply for accessing data in new and compelling situations - even when perfectly
lawful.</p>

<h3>Explicitly blocking user agents:</h3>

<p>HTTP requests include a User-Agent header which is used to specify the
agent requesting a URL.</p>

<p>For example, the Googlebot user agent is:
</p>

<p>
<code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)</code>
</p>

<p>It be difficult to block Googlebot because web traffic would fall off
significantly (as well as revenue for your website).  However, smaller websites
and startups often have their code blocked by websites based on User-Agent.
</p>

<h2>What is rate limiting?  What are IP throttles?</h2>

<p>
<a href="http://en.wikipedia.org/wiki/Rate_limiting">Rate limiting</a> is often
used by websites to ostensibly avoid excessive and expensive resource utilization.
</p>

<p>
It may in fact be used by this in a number of circumstances but by definition,
once you allow anyone to bypass the throttle you have discriminatory access.
</p>

<p>Rate limiting is generally done on an API key which the API requires be
submitted with each request.  </p>

<p>An IP throttle is used to only allow a given IP address a few requests per
hour (or some other arbitrarly duration like per minute).
</p>

<p>IP throttles are especially troubling as they often hurt legitimate companies
but don't hurt companies willing to bypass the IP blocks through controversial
measures.</p>

</div>
    
</div>
</body>
</html>

