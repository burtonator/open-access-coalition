<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
<title>Open Access Coalition</title>
<link rel="stylesheet" href="default.css"></link>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39406709-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

</head>

<body>

<div id="wrap">

<a href="http://openaccesscoalition.org/"><img src="logo.png" width="579" height="169"/></a>

<div class="navbar">
<a href="http://openaccesscoalition.org/">Home</a>
<a href="http://openaccesscoalition.org/faq.html">FAQ</a>
<a href="http://blog.openaccesscoalition.org" target="_new">Weblog</a>
<a href="https://groups.google.com/forum/?fromgroups#!forum/oac-general" target="_new">Group</a>
</div>

<!-- end of header -->

<div id="content">

<h1>Our goal is a free, fair, and open web!</h1>

<a name="open-web"/>

<h2>What is the open web?</h2>

<p>The open web is based on the concept that websites participate fairly with
one another and allow data to be published and accessed without being
blocked by a walled garden of technical obstacles or legal threats. </p>

<p> Without the open web, a start-up like Google, which gathers content across
the whole web, indexes it, and supports search results, would never have been
able to get off the ground. </p>

<p>Much of this user-generated data is published within the public domain,
available under fair use, or Creative Commons license. </p>

<p>Problems arise when publicly viewable data is then selectively made
accessible to some and denied access to by others on a discriminatory basis.</p>

<p>We believe that when data is published in the public, for free, there is an
<a href="implied-license.html">implied license</a> that permits crawlers to access
this content in a fair manner.</p>

<h2>Why should I care about the open web?</h2>

<p>An open web is critical to the future of the Internet and innovation. Data is
the air that web startups breath and without it innovation on the web will be
stiffled.</p>

<p>Users depend on this innovation. The entire social media monitoring industry,
a billion dollar industry, completely relies on open and fair access to web
data.</p>

<p>Users have rights around their own data. They have the right to export this
data, the right to share it with other companies, and the right to benefit when
new companies build innovative products around their data.</p>

<a name="non-discriminatory"/>
<h2>What is non-discriminatory access?</h2>

<p>
A major component to a fair access policy is non-discriminatory access.  It's
paramount that a hosting provider treat each party in an identical manner.  The
access rules must remain consistent if access is to be judged fair and reasonable.
</p>

<p>We have seen web publishers create APIs to build an ecosystem of developers
around their platform of public data, but such access being arbitrarily
terminated to particular parties that become too successful in adjacent
marketplaces that a publishers wants to protect. Manipulating such access to
public data to thwarts innovation and competition, and is inherently detrimental
to both users and society as a whole. </p>

<p> This is a chilling effect because it prevents companies from investing
significantly in an API because they may be cut off at any time. </p>

<p>This is very similar to the way patents are licensed within open standards
using <a href="http://en.wikipedia.org/wiki/Reasonable_and_non-discriminatory_licensing">FRAND (Fair Reasonable and Non-discriminatory Licensing)</a>
</p>

<p>
<blockquote>
Non-discriminatory relates to both the terms and the rates included in licensing
agreements. As the name suggests this commitment requires that licensors treat
each individual licensee in a similar manner. This does not mean that the rates
and payment terms can't change dependent on the volume and creditworthiness of
the licensee. However it does mean that the underlying licensing condition
included in a licensing agreement must be the same regardless of the licensee.
This obligation is included in order to maintain a level playing field with
respect to existing competitors and to ensure that potential new entrants are
free to enter the market on the same basis.
</blockquote>
</p>

<p> This often happens with web crawlers where data is given to Google and
Microsoft but not to anyone else. </p>

<p>
Without non-discriminatory access, the web is not open.
</p>

<h2>Don't these websites OWN the data?</h2>

<p>Yes and no.  Very few websites own the data submitted by the user.  They often
request permissions to use the data within their application but do not attempt to
assert exclusive copyright on their user generated content.</p>

<p>The copyright on the data is owned by the user and often under the public
domain, a Creative Commons license, or available for access under doctrines of
fair use or implied license.</p>

<p>Furthermore, some particularly unfair (but legal) TOUs are written in such
voluminous and dense legalize that no one can understand just what is and is
not permissible regarding access to data or interoperation on a site. The TOUs
can then be arbitrarily interpreted in any way the publisher so deems --
ensuring that equally arbitrary termination, blocking, or suing are all chilling
threats held over the heads of third parties that wish to work with the data of
a particular site.</p>

<p>Recently, <a href="https://www.eff.org/deeplinks/2012/08/good-news-craigslist-drops-exclusive-license-your-posts">Craigslist decided to (shortly) assert exclusive copyright on user
generated content</a> but then did the right thing and backed down from that position:</p>

<p>
<blockquote>

<p>
 In a welcome course correction, craigslist has removed its short-lived provision that required users to grant it an exclusive license to--in other words granting them ownership of--every post. We were unhappily surprised to see this click-through demand, but are glad to see that craigslist has promptly removed it. 
</p>
 
<p>
 For many years, craigslist has been a good digital citizen. Its opposition to SOPA/PIPA was critically important, and it has been at the forefront of challenges to Section 230 and freedom of expression online. We understand that craigslist faces real challenges in trying to preserve its character and does not want third parties to simply reuse its content in ways that are out of line with its user community's expectations and could be harmful to its users. 
</p>

<p>
 Nevertheless, it was important for craigslist to remove the provision because claiming an exclusive license to the user's posts--to the exclusion of everyone, including the original poster--would have harmed both innovation and users' rights, and would have set a terrible precedent. We met with craigslist to discuss this recently and are pleased about their prompt action.  
</p>

</blockquote>
</p>

<a name="resource-requirements"/>

<h2>Don't crawlers use excessive resources when indexing a site?</h2>

<p>No.  Not fair crawlers.</p>

<p>We want to make it clear that we do not support or defend excessive resource
utilization used by misbehaving crawlers and robots. These systems cause havoc
and are very costly to many websites.</p>

<p>However, for large sites, fetching the home page or an RSS feed of public
content imposes a very insignificant impact on the site's performance. For a
website receiving 1M hits per day, fetching the home page once per hour
increases the load on the website by only 0.01%.</p>

<p>Further, if the site were to publish data via
<a href="https://code.google.com/p/pubsubhubbub/">pubsub</a> or an
<a href="http://www.sixapart.com/labs/update/developers/">update stream</a>
the resouce utilization is almost zero.
</p>

<p>For years, <a href="http://sixapart.com">Six Apart</a> was publishing a
stream of updates to anyone who wanted access, free of charge, nearly
spam-free, and with non-discriminatory access to all parties.</p>

<p>They also went further and provided the feed to anyone who wanted it -
immediately.  They also removed private posts from the stream to minimize user
concerns about their private data being indexed by search engines.</p>

<p>
Often, the resource utilization issue is a red herring used as an excuse to
continue unfair access policies.
</p>

<p>Six Apart still provides access to the same feed but have migrated to a more
modern protocol - pubsub.</p>

<h2>Don't hosting providers have the right to publish content under any terms they desire?</h2>

<p>
Yes.  A hosting provider is well within their rights to publish
data under any license they choose.  This could range from being completely unfair <i>or</i>
<b>very</b> fair.
</p>

<p> Users have the right to know in plain terms about the access policies of
sites they use, similar to the way that responsible websites publish clear cut
privacy policies. </p>

<p>Further, the OAC and the user community are free to criticize these sites and
encourage users to take their business elsewhere. </p>

<p>We believe it's in the users best interest to insist on using sites that have
fair access. Additionally, we feel it's vital to the growth of the industry to
have a free, fair, and open Internet. </p>

<h2>What are some common examples that violate fair access?</h2>

<h3>Blocking all crawlers via robots.txt - except Google:</h3>

<p>This happens far too often. This is a problem for new search engines because
they have to make a decision about whether to yield to the robots.txt block,
contact the website owner (which could involve thousands of websites) or
providing a sub-standard experience to their users. Far too often, a decision to
compete in the same way that Google behaves, becomes a contractual, legal, and
even criminal violation.</p>

<h3>Differential pricing based on size, or hidden partnerships:</h3>

<p>There have been situations where API licenses are given based on hidden
partnerships, private investments, or size of company which have prevented open
use of the data.</p>

<p>One example is Twitter's firehose license to Google and Microsoft.</p>

<p>Twitter licensed their firehose to Google (for millions of dollars)
and Google eventually didn't like the terms and <a href="http://allthingsd.com/20110715/with-google-gone-for-now-twitter-tries-to-come-to-terms-with-microsofts-bing/">cancelled their license</a>.
</p>

<p>The issue seems to be that Twitter was attempting to charge Google an
excessive price for a full license for which Google refused.</p>

<h3>Legal harassment:</h3>

<p>There have also been situations where large social networks have harassed
both individuals and small companies with large lawsuits or threat of lawsuit
simply for accessing data in new and compelling situations - even when perfectly
lawful.</p>

<p>These lawsuits can be extremely frighting for small independent
developers:</p>

<p>
In 2010, Pete Warden was <a
href="http://petewarden.typepad.com/searchbrowser/2010/04/how-i-got-sued-by-facebook.html">
threatened with a lawsuit from Facebook</a> for crawling content which they
posted publicly:</p>

<p> <blockquote> I scratched my head a bit and thought "well, how hard can it be
to build my own search engine?". As it turned out, it was very easy. Checking
Facebook's robot.txt, they welcome the web crawlers that search engines use to
gather their data, so I wrote my own in PHP (very similar to this Google Profile
crawler I open-sourced) and left it running for about 6 months. Initially all I
wanted to gather was people's names and locations so I could search on those to
find public profiles. Talking to a few other startups they also needed the same
sort of service so I started looking into either exposing a search API or
sharing that sort of 'phone book for the internet' information with them.
</blockquote> </p>

<p> <blockquote> I noticed Facebook were offering some other interesting
information too, like which pages people were fans of and links to a few of
their friends. I was curious what sort of patterns would emerge if I analyzed
these relationships, so as a side project I set up fanpageanalytics.com to allow
people to explore the data. I was getting more people asking about the data I
was using, so before that went live I emailed Dave Morin at Facebook to give him
a heads-up and check it was all kosher. We'd chatted a little previously, but I
didn't get a reply, and he left the company a month later so my email probably
got lost in the chaos.  </blockquote> </p>

<p>...</p>

<p> <blockquote> On Sunday around 25,000 people read the article, via
YCombinator and Reddit. After that a whole bunch of mainstream news sites picked
it up, and over 150,000 people visited it on Monday. On Tuesday I was hanging
out with my friends at Gnip trying to make sense of it all when my cell phone
rang. It was Facebook's attorney. </blockquote> </p>

<p> <blockquote> He was with the head of their security team, who I knew
slightly because I'd reported several security holes to Facebook over the
years. The attorney said that they were just about to sue me into oblivion, but
in light of my previous good relationship with their security team, they'd give
me one chance to stop the process. They asked and received a verbal assurance
from me that I wouldn't publish the data, and sent me on a letter to sign
confirming that. Their contention was robots.txt had no legal force and they
could sue anyone for accessing their site even if they scrupulously obeyed the
instructions it contained. The only legal way to access any web site with a
crawler was to obtain prior written permission. </blockquote> </p>

<h3>Explicitly blocking user agents:</h3>

<p>HTTP requests include a User-Agent header which is used to specify the
agent requesting a URL.</p>

<p>For example, the Googlebot user agent is:
</p>

<p>
<code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)</code>
</p>

<p>It's difficult to block Googlebot because a site's web traffic would fall off
significantly (as well as revenue). However, smaller websites and startups often
have their crawlers blocked by websites based on User-Agent.</p>

<h2>What is rate limiting?  What are IP throttles?</h2>

<p>
<a href="http://en.wikipedia.org/wiki/Rate_limiting">Rate limiting</a> is often
used by websites to ostensibly avoid excessive and expensive resource utilization.
</p>

<p>
It may in fact be used by this in a number of circumstances but by definition,
once you allow anyone to bypass the throttle you have discriminatory access.
</p>

<p>Rate limiting is generally done on an API is required to be included with
each request. </p>

<p>An IP throttle is similar but designed to only allow a given IP address a few
requests per hour (or some other arbitrarly duration like per minute). </p>

<p>IP throttles are especially troubling as they often hurt legitimate companies
but don't hurt companies willing to bypass the IP blocks through controversial
measures.</p>

<!-- BEGIN of footer -->

<div class="footer">

    <div class="copyright">
        Copyright 2013 Open Access Coalition
    </div>

</div>
</div>

</div>
</body>
</html>
